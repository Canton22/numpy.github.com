<!doctype html><html lang=en data-colorscheme=light><head><meta name=description content="Why NumPy? Powerful n-dimensional arrays. Numerical computing tools. Interoperable. Performant. Open source."><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=x-ua-compatible content="ie=edge"><title>NumPy - Case Study: DeepLabCut 3D Pose Estimation</title>
<link rel=icon href=/images/favicon.ico><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;900"><link rel=stylesheet type=text/css href=/theme-css/code.scss.min.ad03de1683bb39a0d1b31395797b97188e59cda6d778c0671a99db0b4fb799a9.css integrity="sha256-rQPeFoO7OaDRsxOVeXuXGI5ZzabXeMBnGpnbC0+3mak="><link rel=stylesheet type=text/css href=/theme-css/pst/pydata-sphinx-theme.scss.min.0674aae33ec77502f0d130dd952f88cffd1d1929cd7adb47e307d6007f6e1f6f.css integrity="sha256-BnSq4z7HdQLw0TDdlS+Iz/0dGSnNettH4wfWAH9uH28="><link rel=stylesheet type=text/css href=/css/tabs.scss.min.789baca49dcaed7f117290ddfbb257f1622a868114c85b0e0e9dd25b7fcc9199.css integrity="sha256-eJuspJ3K7X8RcpDd+7JX8WIqhoEUyFsODp3SW3/MkZk="><link rel=stylesheet href=/theme-css/bulma.min.6b8b78495833db54dbdc999cafadf06cc7607f94f26fd3667af03364f8b3a9de.css integrity="sha256-a4t4SVgz21Tb3Jmcr63wbMdgf5Tyb9NmevAzZPizqd4="><link rel=stylesheet href=/theme-css/code-highlight.min.d0bd96ff1dbeb4b62536da5935b92af5cd7edb6d6f52b316d721e62078d9f089.css integrity="sha256-0L2W/x2+tLYlNtpZNbkq9c1+221vUrMW1yHmIHjZ8Ik="><link rel=stylesheet href=/theme-css/content.min.971ae9734af45368872e028a4666b2ef915353493128a0f7ac786db98159fa13.css integrity="sha256-lxrpc0r0U2iHLgKKRmay75FTU0kxKKD3rHhtuYFZ+hM="><link rel=stylesheet href=/theme-css/dark-mode.min.1a7d04742ddf658331233b701507a0124657cbf45e02c672c061955181de6dde.css integrity="sha256-Gn0EdC3fZYMxIztwFQegEkZXy/ReAsZywGGVUYHebd4="><link rel=stylesheet href=/theme-css/footer.min.473304cccf1066f31cdb06ca1a948f6a161bfedcb1b2be267a1b25e4609727ba.css integrity="sha256-RzMEzM8QZvMc2wbKGpSPahYb/tyxsr4mehsl5GCXJ7o="><link rel=stylesheet href=/theme-css/fresh.min.40943afb4e58235eea4bbe7fe5f37242a957e7bf144e6d6184103a3bbb22465c.css integrity="sha256-QJQ6+05YI17qS75/5fNyQqlX578UTm1hhBA6O7siRlw="><link rel=stylesheet href=/theme-css/keyfeatures.min.c590db9adf94d7c64c436469d2ac8b3b099fa888a47711e45487e29069b41edb.css integrity="sha256-xZDbmt+U18ZMQ2Rp0qyLOwmfqIikdxHkVIfikGm0Hts="><link rel=stylesheet href=/theme-css/navbar.min.083417499ce4352f0de72f833f1b06f96bb23c4a99aefe4a547dcd6b8cb1e26f.css integrity="sha256-CDQXSZzkNS8N5y+DPxsG+WuyPEqZrv5KVH3Na4yx4m8="><link rel=stylesheet href=/theme-css/news.min.f6f5e88fb4557cf211bb0f8dd3ef68b680557c83dd2c46f9233a2c12ac3e2901.css integrity="sha256-9vXoj7RVfPIRuw+N0+9otoBVfIPdLEb5IzosEqw+KQE="><link rel=stylesheet href=/theme-css/panel.min.9e7a224d47c050a8d5b9137dd6b88b347a6ea27e11bdf415822eff42c8b6f27e.css integrity="sha256-nnoiTUfAUKjVuRN91riLNHpuon4RvfQVgi7/Qsi28n4="><link rel=stylesheet href=/theme-css/posts.min.2265a10f248faf175f6c00dfe3122c60f3bd3541010c88cacac894e6c6dacff3.css integrity="sha256-ImWhDySPrxdfbADf4xIsYPO9NUEBDIjKysiU5sbaz/M="><link rel=stylesheet href=/theme-css/shortcuts.min.5136c95c26c90cd7200682ad6a294a9862e91bebfbdd3bdff33d29434a4f63a5.css integrity="sha256-UTbJXCbJDNcgBoKtailKmGLpG+v73Tvf8z0pQ0pPY6U="><link rel=stylesheet href=/theme-css/styles.min.d46ac3e87210de31c632a6389d01ee969a5865dfe114106681bccba8bce73b67.css integrity="sha256-1GrD6HIQ3jHGMqY4nQHulppYZd/hFBBmgbzLqLznO2c="><link rel=stylesheet href=/theme-css/tables.min.7a44b6bd698323dd3d379b714bd534132e76bf4ba0d3dec61997a8d9ba9db5fb.css integrity="sha256-ekS2vWmDI909N5txS9U0Ey52v0ug097GGZeo2bqdtfs="><link rel=stylesheet href=/theme-css/tabs.min.c11e3ce93c794a16f6e4a3e2481dfdf76728147cdb1fbce255d0e6984c938cbf.css integrity="sha256-wR486Tx5Shb25KPiSB3992coFHzbH7ziVdDmmEyTjL8="><link rel=stylesheet href=/theme-css/teams.min.873dc1b8a8c335913e49e913ccbc7fe19c1ac25a8fa844c223164b317a52b778.css integrity="sha256-hz3BuKjDNZE+SekTzLx/4ZwawlqPqETCIxZLMXpSt3g="><link rel=stylesheet href=/theme-css/vars.min.f6bb17e812a3e9632b27008726ca7ca54a4ca25e5525214597343727cac29039.css integrity="sha256-9rsX6BKj6WMrJwCHJsp8pUpMol5VJSFFlzQ3J8rCkDk="><link rel=stylesheet href=/theme-css/videos.min.b305d98ddc47a74381f484299ab14f9483f69d62928c4ed32697a5701355f138.css integrity="sha256-swXZjdxHp0OB9IQpmrFPlIP2nWKSjE7TJpelcBNV8Tg="><link rel=stylesheet href=/css/casestudies.min.92b0bafc1e58181b02c23f14b861767269e505eadc85a123b4eb79e2527bf2e0.css integrity="sha256-krC6/B5YGBsCwj8UuGF2cmnlBerchaEjtOt54lJ78uA="><link rel=stylesheet href=/css/mailchimp.min.817084b4a116a499dc6e7fbc56c8187ddad405ba37d11e78cfb30369a5440771.css integrity="sha256-gXCEtKEWpJncbn+8VsgYfdrUBbo30R54z7MDaaVEB3E="><link rel=stylesheet href=/css/shell.min.173478d133f6f5990705f3ed2f48714422de15754d813df6aa2a047bf62a51da.css integrity="sha256-FzR40TP29ZkHBfPtL0hxRCLeFXVNgT32qioEe/YqUdo="><script src=https://code.jquery.com/jquery-3.7.1.min.js></script><link rel=alternate hreflang=pt href=/pt/case-studies/deeplabcut-dnn/ title=Português><link rel=alternate hreflang=ja href=/ja/case-studies/deeplabcut-dnn/ title="日本語 (Japanese)"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://numpy.org/images/numpy-image.jpg"><meta name=twitter:title content="Case Study: DeepLabCut 3D Pose Estimation"><meta name=twitter:description content="Analyzing mice hand-movement using DeepLapCut (Source: www.deeplabcut.org )
Open Source Software is accelerating Biomedicine. DeepLabCut enables automated video analysis of animal behavior using Deep Learning. —Alexander Mathis, Assistant Professor, École polytechnique fédérale de Lausanne (EPFL)
About DeepLabCut# DeepLabCut is an open source toolbox that empowers researchers at hundreds of institutions worldwide to track behaviour of laboratory animals, with very little training data, at human-level accuracy. With DeepLabCut technology, scientists can delve deeper into the scientific understanding of motor control and behavior across animal species and timescales."></head><body><nav id=nav class="navbar is-fresh is-transparent no-shadow" role=navigation aria-label="main navigation"><div class="container is-max-widescreen"><div class=navbar-brand><a class=navbar-item href=/><img class=navbar-logo src=/images/logo.svg alt="%!s(<nil>) logo"><div class=navbar-logo-text>NumPy</div></a><a role=button class=navbar-burger aria-label=menu aria-expanded=false data-target=navbar-menu><span aria-hidden=true></span>
<span aria-hidden=true></span>
<span aria-hidden=true></span></a></div><div id=navbar-menu class="navbar-menu is-static"><div class=navbar-end><a href=/install class="navbar-item is-secondary">Install
</a><a href=https://numpy.org/doc/stable class="navbar-item is-secondary">Documentation
</a><a href=/learn class="navbar-item is-secondary">Learn
</a><a href=/community class="navbar-item is-secondary">Community
</a><a href=/about class="navbar-item is-secondary">About Us
</a><a href=/news class="navbar-item is-secondary">News
</a><a href=/contribute class="navbar-item is-secondary">Contribute</a><div class="navbar-item has-dropdown is-hoverable"><a aria-label="Select language" class=navbar-link>English</a><div class=navbar-dropdown><a href=/pt/case-studies/deeplabcut-dnn/ class=navbar-item>Português
</a><a href=/ja/case-studies/deeplabcut-dnn/ class=navbar-item>日本語 (Japanese)</a></div></div></div></div></div></nav><section class="article content-padding"><div class=content-container><ul id=breadcrumbs class=bd-breadcrumbs><li class="breadcrumb-item breadcrumb-home"><a href=/><i class="fas fa-home"></i></a></li><li class=breadcrumb-item><a href=/case-studies/>Case-Studies</a></li><li class=breadcrumb-item><a href=/case-studies/deeplabcut-dnn/>Case Study: DeepLabCut 3D Pose Estimation</a></li></ul><h1>Case Study: DeepLabCut 3D Pose Estimation</h1><div class=article-content><figure class=fig-center><img src=/images/content_images/cs/mice-hand.gif alt=micehandanim><figcaption><p><strong>Analyzing mice hand-movement using DeepLapCut</strong>
<a href=http://www.mousemotorlab.org/deeplabcut><em>(Source: <a href=https://www.deeplabcut.org>www.deeplabcut.org</a> )</em></a></p></figcaption></figure><blockquote cite=https://news.harvard.edu/gazette/story/newsplus/harvard-researchers-awarded-czi-open-source-award/><p>Open Source Software is accelerating Biomedicine. DeepLabCut enables automated video analysis of animal behavior using Deep Learning.</p><p class=attribution>—Alexander Mathis, <em>Assistant Professor, École polytechnique fédérale de Lausanne</em> (<a href=https://www.epfl.ch/en/>EPFL</a>)</p></blockquote><h2 id=about-deeplabcut>About DeepLabCut<a class=headerlink href=#about-deeplabcut title="Link to this heading">#</a></h2><p><a href=https://github.com/DeepLabCut/DeepLabCut>DeepLabCut</a> is an open source toolbox that empowers researchers at hundreds of institutions worldwide to track behaviour of laboratory animals, with very little training data, at human-level accuracy. With DeepLabCut technology, scientists can delve deeper into the scientific understanding of motor control and behavior across animal species and timescales.</p><p>Several areas of research, including neuroscience, medicine, and biomechanics, use data from tracking animal movement. DeepLabCut helps in understanding what humans and other animals are doing by parsing actions that have been recorded on film. Using automation for laborious tasks of tagging and monitoring, along with deep neural network based data analysis, DeepLabCut makes scientific studies involving observing animals, such as primates, mice, fish, flies etc., much faster and more accurate.</p><figure class=fig-center><img src=/images/content_images/cs/race-horse.gif alt=horserideranim><figcaption><p><strong>Colored dots track the positions of a racehorse’s body part</strong><em>(Source: Mackenzie Mathis)</em></p></figcaption></figure><p>DeepLabCut&rsquo;s non-invasive behavioral tracking of animals by extracting the poses of animals is crucial for scientific pursuits in domains such as biomechanics, genetics, ethology & neuroscience. Measuring animal poses non-invasively from video - without markers - in dynamically changing backgrounds is computationally challenging, both technically as well as in terms of resource needs and training data required.</p><p>DeepLabCut allows researchers to estimate the pose of the subject, efficiently enabling them to quantify the behavior through a Python based software toolkit. With DeepLabCut, researchers can identify distinct frames from videos, digitally label specific body parts in a few dozen frames with a tailored GUI, and then the deep learning based pose estimation architectures in DeepLabCut learn how to pick out those same features in the rest of the video and in other similar videos of animals. It works across species of animals, from common laboratory animals such as flies and mice to more unusual animals like <a href=https://www.technologynetworks.com/neuroscience/articles/interview-a-deeper-cut-into-behavior-with-mackenzie-mathis-327618>cheetahs</a>.</p><p>DeepLabCut uses a principle called <a href=https://arxiv.org/pdf/1909.11229>transfer learning</a>, which greatly reduces the amount of training data required and speeds up the convergence of the training period. Depending on the needs, users can pick different network architectures that provide faster inference (e.g. MobileNetV2), which can also be combined with real-time experimental feedback. DeepLabCut originally used the feature detectors from a top-performing human pose estimation architecture, called <a href=https://arxiv.org/abs/1605.03170>DeeperCut</a>, which inspired the name. The package now has been significantly changed to include additional architectures, augmentation methods, and a full front-end user experience. Furthermore, to support large-scale biological experiments DeepLabCut provides active learning capabilities so that users can increase the training set over time to cover edge cases and make their pose estimation algorithm robust within the specific context.</p><p>Recently, the <a href=http://www.mousemotorlab.org/dlc-modelzoo>DeepLabCut model zoo</a> was introduced, which provides pre-trained models for various species and experimental conditions from facial analysis in primates to dog posture. This can be run for instance in the cloud without any labeling of new data, or neural network training, and no programming experience is necessary.</p><h3 id=key-goals-and-results>Key Goals and Results<a class=headerlink href=#key-goals-and-results title="Link to this heading">#</a></h3><ul><li><p><strong>Automation of animal pose analysis for scientific studies:</strong></p><p>The primary objective of DeepLabCut technology is to measure and track posture
of animals in a diverse settings. This data can be used, for example, in
neuroscience studies to understand how the brain controls movement, or to
elucidate how animals socially interact. Researchers have observed a
<a href=https://www.biorxiv.org/content/10.1101/457242v1>tenfold performance boost</a>
with DeepLabCut. Poses can be inferred offline at up to 1200 frames per second
(FPS).</p></li><li><p><strong>Creation of an easy-to-use Python toolkit for pose estimation:</strong></p><p>DeepLabCut wanted to share their animal pose-estimation technology in the form
of an easy to use tool that can be adopted by researchers easily. So they have
created a complete, easy-to-use Python toolbox with project management features
as well. These enable not only automation of pose-estimation but also
managing the project end-to-end by helping the DeepLabCut Toolkit user right
from the dataset collection stage to creating shareable and reusable analysis
pipelines.</p><p>Their <a href=https://github.com/DeepLabCut/DeepLabCut>toolkit</a> is now available as open source.</p><p>A typical DeepLabCut Workflow includes:</p><ul><li>creation and refining of training sets via active learning</li><li>creation of tailored neural networks for specific animals and scenarios</li><li>code for large-scale inference on videos</li><li>draw inferences using integrated visualization tools</li></ul></li></ul><figure class=csfigcaption><img src=/images/content_images/cs/deeplabcut-toolkit-steps.png alt=dlcsteps><figcaption><p><strong>Pose estimation steps with DeepLabCut</strong>
<a href=https://twitter.com/DeepLabCut/status/1198046918284210176/photo/1>(Source: DeepLabCut)</a></p></figcaption></figure><h3 id=the-challenges>The Challenges<a class=headerlink href=#the-challenges title="Link to this heading">#</a></h3><ul><li><p><strong>Speed</strong></p><p>Fast processing of animal behavior videos in order to measure their behavior
and at the same time make scientific experiments more efficient, accurate.
Extracting detailed animal poses for laboratory experiments, without
markers, in dynamically changing backgrounds, can be challenging, both
technically as well as in terms of resource needs and training data required.
Coming up with a tool that is easy to use without the need for skills such
as computer vision expertise that enables scientists to do research in more
real-world contexts, is a non-trivial problem to solve.</p></li><li><p><strong>Combinatorics</strong></p><p>Combinatorics involves assembly and integration of movement of multiple
limbs into individual animal behavior. Assembling keypoints and their
connections into individual animal movements and linking them across time
is a complex process that requires heavy-duty numerical analysis, especially
in case of multi-animal movement tracking in experiment videos.</p></li><li><p><strong>Data Processing</strong></p><p>Last but not the least, array manipulation - processing large stacks of
arrays corresponding to various images, target tensors and keypoints is
fairly challenging.</p></li></ul><figure class=csfigcaption><img src=/images/content_images/cs/pose-estimation.png alt=challengesfig><figcaption><p><strong>Pose estimation variety and complexity</strong>
<a href=https://www.biorxiv.org/content/10.1101/476531v1.full.pdf>(Source: Mackenzie Mathis)</a></p></figcaption></figure><h2 id=numpys-role-in-meeting-pose-estimation-challenges>NumPy&rsquo;s Role in meeting Pose Estimation Challenges<a class=headerlink href=#numpys-role-in-meeting-pose-estimation-challenges title="Link to this heading">#</a></h2><p>NumPy addresses DeepLabCut technology&rsquo;s core need of numerical computations at
high speed for behavioural analytics. Besides NumPy, DeepLabCut employs
various Python software that utilize NumPy at their core, such as
<a href=https://www.scipy.org>SciPy</a>, <a href=https://pandas.pydata.org>Pandas</a>,
<a href=https://matplotlib.org>matplotlib</a>,
<a href=https://github.com/tensorpack/tensorpack>Tensorpack</a>,
<a href=https://github.com/aleju/imgaug>imgaug</a>,
<a href=https://scikit-learn.org/stable/>scikit-learn</a>,
<a href=https://scikit-image.org>scikit-image</a> and
<a href=https://www.tensorflow.org>Tensorflow</a>.</p><p>The following features of NumPy played a key role in addressing the image
processing, combinatorics requirements and need for fast computation in
DeepLabCut pose estimation algorithms:</p><ul><li>Vectorization</li><li>Masked Array Operations</li><li>Linear Algebra</li><li>Random Sampling</li><li>Reshaping of large arrays</li></ul><p>DeepLabCut utilizes NumPy’s array capabilities throughout the workflow offered
by the toolkit. In particular, NumPy is used for sampling distinct frames for
human annotation labeling, and for writing, editing and processing annotation
data. Within TensorFlow the neural network is trained by DeepLabCut technology
over thousands of iterations to predict the ground truth annotations from
frames. For this purpose, target densities (scoremaps) are created to cast pose
estimation as a image-to-image translation problem. To make the neural networks
robust, data augmentation is employed, which requires the calculation of target
scoremaps subject to various geometric and image processing steps. To make
training fast, NumPy’s vectorization capabilities are leveraged. For inference,
the most likely predictions from target scoremaps need to extracted and one
needs to efficiently “link predictions to assemble individual animals”.</p><figure class=fig-center><img src=/images/content_images/cs/deeplabcut-workflow.png alt=workflow><figcaption><p><strong>DeepLabCut Workflow</strong>
<a href=https://www.researchgate.net/figure/DeepLabCut-work-flow-The-diagram-delineates-the-work-flow-as-well-as-the-directory-and_fig1_329185962><em>(Source: Mackenzie Mathis)</em></a></p></figcaption></figure><h2 id=summary>Summary<a class=headerlink href=#summary title="Link to this heading">#</a></h2><p>Observing and efficiently describing behavior is a core tenant of modern
ethology, neuroscience, medicine, and technology.
<a href=http://orga.cvss.cc/wp-content/uploads/2019/05/NathMathis2019.pdf>DeepLabCut</a>
allows researchers to estimate the pose of the subject, efficiently enabling
them to quantify the behavior. With only a small set of training images,
the DeepLabCut Python toolbox allows training a neural network to within human
level labeling accuracy, thus expanding its application to not only behavior
analysis in the laboratory, but to potentially also in sports, gait analysis,
medicine and rehabilitation studies. Complex combinatorics, data processing
challenges faced by DeepLabCut algorithms are addressed through the use of
NumPy&rsquo;s array manipulation capabilities.</p><figure class=fig-center><img src=/images/content_images/cs/numpy_dlc_benefits.png alt="numpy benefits"><figcaption><p><strong>Key NumPy Capabilities utilized</strong></p></figcaption></figure></div></div><div id=shortcuts-container><div id=shortcuts><div id=shortcuts-header><i class="fa-solid fa-list"></i> On this page</div></div></div></section><div id=backtotop><a href=# id=backtotop-color></a></div><footer id=footer><div class="container is-max-widescreen"><div id=footer-columns class=columns><div id=footer-logo-column><img id=footer-logo src=/images/logo.svg alt="NumPy logo. "></div><div class=footer-column><div class=footer-item><a href=/install>Install</a></div><div class=footer-item><a href=https://numpy.org/doc/stable>Documentation</a></div><div class=footer-item><a href=/learn>Learn</a></div><div class=footer-item><a href=/citing-numpy>Citing Numpy</a></div><div class=footer-item><a href=https://numpy.org/neps/roadmap.html>Roadmap</a></div></div><div class=footer-column><div class=footer-item><a href=/about>About us</a></div><div class=footer-item><a href=/community>Community</a></div><div class=footer-item><a href=/user-surveys>User surveys</a></div><div class=footer-item><a href=/contribute>Contribute</a></div><div class=footer-item><a href=/code-of-conduct>Code of conduct</a></div></div><div class=footer-column><div class=footer-item><a href=/gethelp>Get help</a></div><div class=footer-item><a href=/terms>Terms of use</a></div><div class=footer-item><a href=/privacy>Privacy</a></div><div class=footer-item><a href=/press-kit>Press kit</a></div></div><div class=footer-actions><p>Sign up for the latest NumPy news, resources,<br>and more</p><form action="https://numpy.us4.list-manage.com/subscribe/post?u=5ddd0d1d6e807900a8212481a&amp;id=287fa4253c" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate sign-up-container" target=_blank novalidate><div class=sign-up-image><svg class="icon mail-icon" viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M22 6c0-1.1-.9-2-2-2H4c-1.1.0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1.0 2-.9 2-2V6zm-2 0-8 5-8-5h16zm0 12H4V8l8 5 8-5v10z"/></svg></div><input type=email name=EMAIL class="required email sign-up-input" id=mce-EMAIL aria-label="Input for email, press enter to submit" onkeypress='(event.which===13||event.keyCode===13||event.key==="Enter")&&sendThankYou()'><div class=submission-instructions>Press Enter</div><button class=signup-button onclick=sendThankYou() aria-label=Submit><svg class="icon sent-icon" viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2.01 21 23 12 2.01 3 2 10l15 2-15 2z"/></svg></button><div id=mce-responses class=clear><div class=response id=mce-error-response style=display:none></div><div class=response id=mce-success-response style=display:none></div></div><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_5ddd0d1d6e807900a8212481a_287fa4253c tabindex=-1></div><div class=clear><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class=button style=display:none></div></form><div class=thank-you>Thank you! &#127881;</div><nav class="level is-mobile"><div class=community-icons><a class=level-item href=https://github.com/numpy/numpy aria-label=https://github.com/numpy/numpy><svg class="icon github-icon" viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 .297c-6.63.0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577.0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93.0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176.0.0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22.0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22.0 1.606-.015 2.896-.015 3.286.0.315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class=level-item href=https://www.youtube.com/channel/UCguIL9NZ7ybWK5WQ53qbHng aria-label=https://www.youtube.com/channel/UCguIL9NZ7ybWK5WQ53qbHng><svg class="icon youtube-icon" viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M23.498 6.186A3.016 3.016.0 0021.376 4.05C19.505 3.545 12 3.545 12 3.545s-7.505.0-9.377.505A3.017 3.017.0 00.502 6.186C0 8.07.0 12 0 12s0 3.93.502 5.814a3.016 3.016.0 002.122 2.136c1.871.505 9.376.505 9.376.505s7.505.0 9.377-.505a3.015 3.015.0 002.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/></svg></a><a class=level-item href=https://twitter.com/numpy_team aria-label=https://twitter.com/numpy_team><svg class="icon twitter-icon" viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958.0 002.163-2.723c-.951.555-2.005.959-3.127 1.184A4.92 4.92.0 0011.78 8.288C7.69 8.095 4.067 6.13 1.64 3.162A4.822 4.822.0 00.974 5.637c0 1.71.87 3.213 2.188 4.096A4.904 4.904.0 01.934 9.117v.06a4.923 4.923.0 003.946 4.827 4.996 4.996.0 01-2.212.085 4.936 4.936.0 004.604 3.417A9.867 9.867.0 011.17 19.611c-.39.0-.779-.023-1.17-.067a13.995 13.995.0 007.557 2.209c9.053.0 13.998-7.496 13.998-13.985.0-.21.0-.42-.015-.63A9.935 9.935.0 0024 4.59z"/></svg></a></div></nav><div class=copyright>&copy; 2023 NumPy team. All rights reserved.</div></div></div></div></footer></body><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/js/all.min.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><script type=text/javascript src=/js/bundle.min.js></script><script type=text/javascript>setupShortcuts(maxLevel=2)</script><script defer data-domain=numpy.org src=https://views.scientific-python.org/js/script.js></script></html>